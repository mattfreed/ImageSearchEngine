{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5785 Final Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we load libraries, define our train/test split, and load the word2vec dictionary using gensim\n",
    "\n",
    "How to download the pretrained word2vec representation: run in command line `wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded word vectors successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "num_train = 8000\n",
    "num_dev = 2000\n",
    "num_test = 2000\n",
    "split_idx = list(range(num_train + num_dev))\n",
    "random.shuffle(split_idx)\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "print(\"Loaded word vectors successfully!\")\n",
    "\n",
    "#from gensim.models import Word2Vec\n",
    "#word2vec = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, norm_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Next we parse the descriptions to form the X matrices, along with removing stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built all x matrices!\n",
      "x_train shape: (8000, 300)\n",
      "x_dev shape: (2000, 300)\n",
      "x_test shape: (2000, 300)\n"
     ]
    }
   ],
   "source": [
    "#Look here for preprocessing stuff and cnn stuff: https://stats.stackexchange.com/questions/335836/cnn-architectures-for-regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def parse_descriptions(data_dir, num_doc):\n",
    "    docs = []\n",
    "    for i in range(num_doc):\n",
    "        path = os.path.join(data_dir, \"%d.txt\" % i)\n",
    "        with open(path) as f:\n",
    "            docs.append(f.read())\n",
    "    return docs\n",
    "\n",
    "def doc_to_vec(sentence, word2vec):\n",
    "    # get list of word vectors in sentence\n",
    "    word_vecs = [word2vec.get_vector(w) for w in sentence.split() if w in word2vec.vocab]\n",
    "    # return average\n",
    "\n",
    "    return np.stack(word_vecs).mean(0)\n",
    "\n",
    "# build x matrices\n",
    "train_dev_desc = parse_descriptions(\"descriptions_train\", num_doc=(num_train+num_dev))\n",
    "test_desc = parse_descriptions(\"descriptions_test\", num_doc=num_test)\n",
    "\n",
    "from nltk.stem import PorterStemmer \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "from gensim import corpora, models, similarities\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "\n",
    "# articles = (shared_articles['text']).tolist()\n",
    "articles = (train_dev_desc)\n",
    "\n",
    "\n",
    "stop_words = set( {\"is\", \"A\", \"his\", \"her\", \"them\", \".\", \"!\",\"?\",\":\",\",\",\"that\",\"are\",\"am\",\"there\",\"and\",\"it\",\"its\",\"has\",\"have\",\"will\",\"what\"}) \n",
    "for i in range(len(train_dev_desc)):\n",
    "    sentence = train_dev_desc[i]\n",
    "    word_tokens = word_tokenize(sentence) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    filtered_sentence = [] \n",
    "\n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w)\n",
    "    # print(filtered_sentence)\n",
    "    sentence = ' '.join(word for word in filtered_sentence)\n",
    "    # print(sentence)\n",
    "\n",
    "\n",
    "    tagged_sentence = nltk.tag.pos_tag(sentence.split())\n",
    "    # print(tagged_sentence)\n",
    "    edited_sentence = [word for word,tag in tagged_sentence if tag != 'IN' and tag != 'DT' and tag != 'TO' and tag != 'RBR'and tag != 'RBS']\n",
    "    edited_sentence = ' '.join(word for word in edited_sentence)\n",
    "    train_dev_desc[i] = edited_sentence\n",
    "\n",
    "for i in range(len(test_desc)):\n",
    "    sentence = test_desc[i]\n",
    "    word_tokens = word_tokenize(sentence) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    filtered_sentence = [] \n",
    "\n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w)\n",
    "    # print(filtered_sentence)\n",
    "    sentence = ' '.join(word for word in filtered_sentence)\n",
    "    # print(sentence)\n",
    "\n",
    "\n",
    "    tagged_sentence = nltk.tag.pos_tag(sentence.split())\n",
    "    # print(tagged_sentence)\n",
    "    edited_sentence = [word for word,tag in tagged_sentence if tag != 'IN' and tag != 'DT' and tag != 'TO' and tag != 'RBR'and tag != 'RBS']\n",
    "    edited_sentence = ' '.join(word for word in edited_sentence)\n",
    "    test_desc[i] = edited_sentence\n",
    "\n",
    "x_train = np.array([doc_to_vec(train_dev_desc[i], word2vec) for i in split_idx[:num_train]])\n",
    "x_dev = np.array([doc_to_vec(train_dev_desc[i], word2vec) for i in split_idx[num_train:]])\n",
    "x_test = np.array([doc_to_vec(d, word2vec) for d in test_desc])\n",
    "\n",
    "\n",
    "print(\"Built all x matrices!\")\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_dev shape:\", x_dev.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next we parse the files to form the Y matrices, along with concatenating the feature vectores and the tags to create the full Y matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "room bed furniture property bedroom bunk bed house building interior design hostel furniture bed accessory backpack suitcase tie\n",
      "Built all x matrices!\n",
      "x_train shape: (16000, 300)\n",
      "x_dev shape: (4000, 300)\n",
      "x_test shape: (2000, 300)\n",
      "Built all y matrices!\n",
      "y_train shape: (16000, 3348)\n",
      "y_dev shape: (4000, 3348)\n",
      "y_test shape: (2000, 3348)\n"
     ]
    }
   ],
   "source": [
    "def parse_labels(data_dir, num_doc):\n",
    "    docs = []\n",
    "    for i in range(num_doc):\n",
    "        path = os.path.join(data_dir, \"%d.txt\" % i)\n",
    "        with open(path,'rb') as f:\n",
    "            docs.append(f.read())\n",
    "    return docs\n",
    "\n",
    "\n",
    "tags_train = parse_labels(\"aml_final/google_labels\", num_doc=(num_train+num_dev))\n",
    "tags_test = parse_labels(\"aml_final/google_labels_test\", num_doc=num_test)\n",
    "\n",
    "for i in range(len(tags_train)):\n",
    "    tags_train[i] = tags_train[i].decode('utf8','ignore').lower()\n",
    "    if not tags_train[i]:\n",
    "        tags_train[i] = 'none'\n",
    "\n",
    "for i in range(len(tags_test)):\n",
    "    tags_test[i] = tags_test[i].decode('utf8', 'ignore').lower()\n",
    "    if not tags_test[i]:\n",
    "        tags_test[i] = 'none'\n",
    "\n",
    "tags_train[2039] = \"kitchen\"\n",
    "tags_train[5216] = 'shirt'\n",
    "tags_train[9017] = 'person cat'\n",
    "\n",
    "\n",
    "##################tags official##############\n",
    "def remove_duplicate_words(s):\n",
    "    return ' '.join(dict.fromkeys(s.split()))\n",
    "\n",
    "tags_official_train = parse_descriptions('tags_train', num_doc=num_train + num_dev)\n",
    "tags_official_test = parse_descriptions('tags_test', num_doc=num_test)\n",
    "\n",
    "for i in range(len(tags_official_train)):\n",
    "    tags_official_train[i] = tags_official_train[i].replace(':',' ')\n",
    "    tags_official_train[i] = tags_official_train[i].replace(\"\\n\", \" \")\n",
    "    tags_official_train[i] = remove_duplicate_words(tags_official_train[i])\n",
    "    tags_train[i] = tags_train[i] + tags_official_train[i]\n",
    "    \n",
    "for i in range(len(tags_official_test)):\n",
    "    tags_official_test[i] = tags_official_test[i].replace(':',' ')\n",
    "    tags_official_test[i] = tags_official_test[i].replace(\"\\n\", \" \")\n",
    "    tags_official_test[i] = remove_duplicate_words(tags_official_test[i])\n",
    "    tags_test[i] = tags_test[i] + tags_official_test[i]\n",
    "########################tags\n",
    "print(tags_test[0])\n",
    "# for i in range(num_dev): #these are images google couldnt label\n",
    "#     if not tags_train[i]:\n",
    "#         tags_train[i] = ' '\n",
    "#     if not tags_test[i]:\n",
    "#         tags_test[i] = ' '\n",
    "        \n",
    "stop_words = set( {\"is\", \"A\", \"his\", \"her\", \"them\", \".\", \"!\",\"?\",\":\",\",\",\"that\",\"are\",\"am\",\"there\",\"and\",\"it\",\"its\",\"has\",\"have\",\"will\",\"what\"}) \n",
    "for i in range(len(tags_train)):\n",
    "    sentence = tags_train[i]\n",
    "    word_tokens = word_tokenize(sentence) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    filtered_sentence = [] \n",
    "\n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w)\n",
    "    # print(filtered_sentence)\n",
    "    sentence = ' '.join(word for word in filtered_sentence)\n",
    "    # print(sentence)\n",
    "\n",
    "\n",
    "    tagged_sentence = nltk.tag.pos_tag(sentence.split())\n",
    "    # print(tagged_sentence)\n",
    "    edited_sentence = [word for word,tag in tagged_sentence if tag != 'IN' and tag != 'DT' and tag != 'TO' and tag != 'RBR'and tag != 'RBS']\n",
    "    edited_sentence = ' '.join(word for word in edited_sentence)\n",
    "    tags_train[i] = edited_sentence\n",
    "\n",
    "for i in range(len(tags_test)):\n",
    "    sentence = tags_test[i]\n",
    "    word_tokens = word_tokenize(sentence) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    filtered_sentence = [] \n",
    "\n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w)\n",
    "    # print(filtered_sentence)\n",
    "    sentence = ' '.join(word for word in filtered_sentence)\n",
    "    # print(sentence)\n",
    "\n",
    "\n",
    "    tagged_sentence = nltk.tag.pos_tag(sentence.split())\n",
    "    # print(tagged_sentence)\n",
    "    edited_sentence = [word for word,tag in tagged_sentence if tag != 'IN' and tag != 'DT' and tag != 'TO' and tag != 'RBR'and tag != 'RBS']\n",
    "    edited_sentence = ' '.join(word for word in edited_sentence)\n",
    "    tags_test[i] = edited_sentence\n",
    "\n",
    "    \n",
    "x_tr = np.array([doc_to_vec(tags_train[i], word2vec) for i in split_idx[:num_train]])\n",
    "x_d = np.array([doc_to_vec(tags_train[i], word2vec) for i in split_idx[num_train:]])\n",
    "x_t = np.array([doc_to_vec(d, word2vec) for d in tags_test])\n",
    "\n",
    "y_train = np.append(x_tr,x_train,axis=0)\n",
    "y_dev = np.append(x_d,x_dev,axis=0)\n",
    "# y_test = np.append(x_t,x_test,axis=0)\n",
    "\n",
    "\n",
    "x_train = np.append(x_train, x_tr, axis=0)\n",
    "x_dev = np.append(x_dev, x_d, axis=0)\n",
    "\n",
    "\n",
    "##Parse Resnet\n",
    "#############################3\n",
    "def parse_features(features_path):\n",
    "    vec_map = {}\n",
    "    with open(features_path) as f:\n",
    "        for row in csv.reader(f):\n",
    "            img_id = int(row[0].split(\"/\")[1].split(\".\")[0])\n",
    "            vec_map[img_id] = np.array([float(x) for x in row[1:]])\n",
    "    return np.array([v for k, v in sorted(vec_map.items())])\n",
    "\n",
    "# build y matrices\n",
    "p = np.random.randn(1000, 1000)\n",
    "y_train_dev = parse_features(\"features_train/features_resnet1000_train.csv\") @ p\n",
    "y_train2 = y_train_dev[split_idx[:num_train]]\n",
    "y_dev2 = y_train_dev[split_idx[num_train:]]\n",
    "y_test = parse_features(\"features_test/features_resnet1000_test.csv\") @ p\n",
    "\n",
    "y_train2 =  np.append(y_train2, y_train2, axis=0)\n",
    "y_dev2 =  np.append(y_dev2, y_dev2, axis=0)\n",
    "\n",
    "##Append the tags and the features\n",
    "y_train = np.append(y_train2, y_train, axis=1)\n",
    "y_dev = np.append(y_dev2, y_dev, axis=1)\n",
    "y_test = np.append(y_test, x_t, axis=1)\n",
    "\n",
    "##Add in the additional higher level resnet features\n",
    "y_train_dev2 = parse_features(\"features_train/features_resnet1000intermediate_train.csv\")\n",
    "y_train3 = y_train_dev2[split_idx[:num_train]]\n",
    "y_dev3 = y_train_dev2[split_idx[num_train:]]\n",
    "y_test2 = parse_features(\"features_test/features_resnet1000intermediate_test.csv\")\n",
    "\n",
    "y_train3 =  np.append(y_train3, y_train3, axis=0)\n",
    "y_dev3 =  np.append(y_dev3, y_dev3, axis=0)\n",
    "\n",
    "\n",
    "##Append the old features and the new features\n",
    "y_train = np.append(y_train,y_train3, axis=1)\n",
    "y_dev = np.append(y_dev,y_dev3, axis=1)\n",
    "y_test = np.append(y_test, y_test2, axis=1)\n",
    "\n",
    "\n",
    "##############\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_dev  = scaler.fit_transform(x_dev)\n",
    "x_test  = scaler.fit_transform(x_test)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "y_train = scaler.fit_transform(y_train)\n",
    "y_dev  = scaler.fit_transform(y_dev)\n",
    "y_test  = scaler.fit_transform(y_test)\n",
    "   \n",
    "\n",
    "print(\"Built all x matrices!\")\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_dev shape:\", x_dev.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "\n",
    "print(\"Built all y matrices!\")\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_dev shape:\", y_dev.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### In addition we parse the ResNet features to form the Y matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build CNN below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 6696)              2015496   \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 6696)              0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 3348)              22421556  \n",
      "=================================================================\n",
      "Total params: 24,437,052\n",
      "Trainable params: 24,437,052\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/150\n",
      "16000/16000 [==============================] - 12s 733us/step - loss: 19.0296 - val_loss: 15.5133\n",
      "Epoch 2/150\n",
      "16000/16000 [==============================] - 11s 703us/step - loss: 12.5549 - val_loss: 8.3823\n",
      "Epoch 3/150\n",
      "16000/16000 [==============================] - 11s 705us/step - loss: 6.8996 - val_loss: 4.7484\n",
      "Epoch 4/150\n",
      "16000/16000 [==============================] - 11s 697us/step - loss: 3.9349 - val_loss: 2.0816\n",
      "Epoch 5/150\n",
      "16000/16000 [==============================] - 11s 704us/step - loss: 2.1650 - val_loss: 1.6985\n",
      "Epoch 6/150\n",
      "16000/16000 [==============================] - 11s 698us/step - loss: 1.8423 - val_loss: 1.3214\n",
      "Epoch 7/150\n",
      "16000/16000 [==============================] - 11s 704us/step - loss: 1.5393 - val_loss: 1.1358\n",
      "Epoch 8/150\n",
      "16000/16000 [==============================] - 11s 692us/step - loss: 1.3005 - val_loss: 0.9837\n",
      "Epoch 9/150\n",
      "16000/16000 [==============================] - 11s 703us/step - loss: 1.1065 - val_loss: 0.8797\n",
      "Epoch 10/150\n",
      "16000/16000 [==============================] - 11s 703us/step - loss: 0.9678 - val_loss: 0.8205\n",
      "Epoch 11/150\n",
      "16000/16000 [==============================] - 11s 707us/step - loss: 0.8793 - val_loss: 0.7680\n",
      "Epoch 12/150\n",
      "16000/16000 [==============================] - 11s 706us/step - loss: 0.8157 - val_loss: 0.7347\n",
      "Epoch 13/150\n",
      "16000/16000 [==============================] - 11s 702us/step - loss: 0.7726 - val_loss: 0.7077\n",
      "Epoch 14/150\n",
      "16000/16000 [==============================] - 11s 713us/step - loss: 0.7416 - val_loss: 0.6988\n",
      "Epoch 15/150\n",
      "16000/16000 [==============================] - 11s 702us/step - loss: 0.7205 - val_loss: 0.6808\n",
      "Epoch 16/150\n",
      "16000/16000 [==============================] - 11s 709us/step - loss: 0.7028 - val_loss: 0.6726\n",
      "Epoch 17/150\n",
      "16000/16000 [==============================] - 11s 700us/step - loss: 0.6894 - val_loss: 0.6648\n",
      "Epoch 18/150\n",
      "16000/16000 [==============================] - 11s 707us/step - loss: 0.6786 - val_loss: 0.6595\n",
      "Epoch 19/150\n",
      "16000/16000 [==============================] - 11s 709us/step - loss: 0.6714 - val_loss: 0.6536\n",
      "Epoch 20/150\n",
      "16000/16000 [==============================] - 11s 701us/step - loss: 0.6638 - val_loss: 0.6497\n",
      "Epoch 21/150\n",
      "16000/16000 [==============================] - 11s 711us/step - loss: 0.6582 - val_loss: 0.6452\n",
      "Epoch 22/150\n",
      "16000/16000 [==============================] - 11s 704us/step - loss: 0.6527 - val_loss: 0.6422\n",
      "Epoch 23/150\n",
      "16000/16000 [==============================] - 11s 702us/step - loss: 0.6483 - val_loss: 0.6386\n",
      "Epoch 24/150\n",
      "16000/16000 [==============================] - 11s 703us/step - loss: 0.6439 - val_loss: 0.6357\n",
      "Epoch 25/150\n",
      "16000/16000 [==============================] - 11s 704us/step - loss: 0.6396 - val_loss: 0.6331\n",
      "Epoch 26/150\n",
      "16000/16000 [==============================] - 11s 702us/step - loss: 0.6359 - val_loss: 0.6310\n",
      "Epoch 27/150\n",
      "16000/16000 [==============================] - 11s 701us/step - loss: 0.6324 - val_loss: 0.6287\n",
      "Epoch 28/150\n",
      "16000/16000 [==============================] - 11s 707us/step - loss: 0.6296 - val_loss: 0.6265\n",
      "Epoch 29/150\n",
      "16000/16000 [==============================] - 11s 699us/step - loss: 0.6266 - val_loss: 0.6249\n",
      "Epoch 30/150\n",
      "16000/16000 [==============================] - 11s 714us/step - loss: 0.6235 - val_loss: 0.6231\n",
      "Epoch 31/150\n",
      "16000/16000 [==============================] - 11s 706us/step - loss: 0.6207 - val_loss: 0.6215\n",
      "Epoch 32/150\n",
      "16000/16000 [==============================] - 11s 699us/step - loss: 0.6181 - val_loss: 0.6202\n",
      "Epoch 33/150\n",
      "16000/16000 [==============================] - 11s 712us/step - loss: 0.6157 - val_loss: 0.6190\n",
      "Epoch 34/150\n",
      "16000/16000 [==============================] - 11s 706us/step - loss: 0.6135 - val_loss: 0.6177\n",
      "Epoch 35/150\n",
      "16000/16000 [==============================] - 11s 701us/step - loss: 0.6112 - val_loss: 0.6164\n",
      "Epoch 36/150\n",
      "16000/16000 [==============================] - 11s 703us/step - loss: 0.6090 - val_loss: 0.6155\n",
      "Epoch 37/150\n",
      "16000/16000 [==============================] - 11s 717us/step - loss: 0.6071 - val_loss: 0.6141\n",
      "Epoch 38/150\n",
      "16000/16000 [==============================] - 11s 700us/step - loss: 0.6050 - val_loss: 0.6130\n",
      "Epoch 39/150\n",
      "16000/16000 [==============================] - 11s 712us/step - loss: 0.6029 - val_loss: 0.6121\n",
      "Epoch 40/150\n",
      "16000/16000 [==============================] - 11s 704us/step - loss: 0.6012 - val_loss: 0.6110\n",
      "Epoch 41/150\n",
      "16000/16000 [==============================] - 11s 710us/step - loss: 0.5990 - val_loss: 0.6102\n",
      "Epoch 42/150\n",
      "16000/16000 [==============================] - 11s 710us/step - loss: 0.5974 - val_loss: 0.6093\n",
      "Epoch 43/150\n",
      "16000/16000 [==============================] - 11s 704us/step - loss: 0.5957 - val_loss: 0.6086\n",
      "Epoch 44/150\n",
      "16000/16000 [==============================] - 11s 703us/step - loss: 0.5939 - val_loss: 0.6078\n",
      "Epoch 45/150\n",
      "16000/16000 [==============================] - 11s 704us/step - loss: 0.5920 - val_loss: 0.6071\n",
      "Epoch 46/150\n",
      "16000/16000 [==============================] - 11s 712us/step - loss: 0.5903 - val_loss: 0.6063\n",
      "Epoch 47/150\n",
      "16000/16000 [==============================] - 11s 717us/step - loss: 0.5890 - val_loss: 0.6057\n",
      "Epoch 48/150\n",
      "16000/16000 [==============================] - 11s 699us/step - loss: 0.5872 - val_loss: 0.6051\n",
      "Epoch 49/150\n",
      "16000/16000 [==============================] - 11s 701us/step - loss: 0.5856 - val_loss: 0.6042\n",
      "Epoch 50/150\n",
      "16000/16000 [==============================] - 11s 705us/step - loss: 0.5842 - val_loss: 0.6037\n",
      "Epoch 51/150\n",
      "16000/16000 [==============================] - 11s 694us/step - loss: 0.5824 - val_loss: 0.6031\n",
      "Epoch 52/150\n",
      "16000/16000 [==============================] - 11s 704us/step - loss: 0.5814 - val_loss: 0.6028\n",
      "Epoch 53/150\n",
      "16000/16000 [==============================] - 11s 715us/step - loss: 0.5798 - val_loss: 0.6020\n",
      "Epoch 54/150\n",
      "16000/16000 [==============================] - 11s 704us/step - loss: 0.5786 - val_loss: 0.6016\n",
      "Epoch 55/150\n",
      "16000/16000 [==============================] - 11s 712us/step - loss: 0.5768 - val_loss: 0.6010\n",
      "Epoch 56/150\n",
      "16000/16000 [==============================] - 11s 714us/step - loss: 0.5757 - val_loss: 0.6001\n",
      "Epoch 57/150\n",
      "16000/16000 [==============================] - 11s 711us/step - loss: 0.5738 - val_loss: 0.5998\n",
      "Epoch 58/150\n",
      "16000/16000 [==============================] - 11s 707us/step - loss: 0.5726 - val_loss: 0.5997\n",
      "Epoch 59/150\n",
      "16000/16000 [==============================] - 11s 696us/step - loss: 0.5713 - val_loss: 0.5996\n",
      "Epoch 60/150\n",
      "16000/16000 [==============================] - 11s 707us/step - loss: 0.5703 - val_loss: 0.5991\n",
      "Epoch 61/150\n",
      "16000/16000 [==============================] - 11s 705us/step - loss: 0.5687 - val_loss: 0.5984\n",
      "Epoch 62/150\n",
      "16000/16000 [==============================] - 11s 707us/step - loss: 0.5675 - val_loss: 0.5981\n",
      "Epoch 63/150\n",
      "16000/16000 [==============================] - 11s 714us/step - loss: 0.5664 - val_loss: 0.5977\n",
      "Epoch 64/150\n",
      "16000/16000 [==============================] - 11s 711us/step - loss: 0.5651 - val_loss: 0.5973\n",
      "Epoch 65/150\n",
      "16000/16000 [==============================] - 11s 703us/step - loss: 0.5638 - val_loss: 0.5969\n",
      "Epoch 66/150\n",
      "16000/16000 [==============================] - 11s 700us/step - loss: 0.5627 - val_loss: 0.5964\n",
      "Epoch 67/150\n",
      "16000/16000 [==============================] - 11s 709us/step - loss: 0.5615 - val_loss: 0.5965\n",
      "Epoch 68/150\n",
      "16000/16000 [==============================] - 11s 702us/step - loss: 0.5601 - val_loss: 0.5959\n",
      "Epoch 69/150\n",
      "16000/16000 [==============================] - 11s 695us/step - loss: 0.5590 - val_loss: 0.5957\n",
      "Epoch 70/150\n",
      "16000/16000 [==============================] - 11s 705us/step - loss: 0.5582 - val_loss: 0.5955\n",
      "Epoch 71/150\n",
      "16000/16000 [==============================] - 11s 697us/step - loss: 0.5568 - val_loss: 0.5950\n",
      "Epoch 72/150\n",
      "16000/16000 [==============================] - 11s 701us/step - loss: 0.5558 - val_loss: 0.5948\n",
      "Epoch 73/150\n",
      "16000/16000 [==============================] - 11s 701us/step - loss: 0.5546 - val_loss: 0.5946\n",
      "Epoch 74/150\n",
      "16000/16000 [==============================] - 12s 723us/step - loss: 0.5534 - val_loss: 0.5945\n",
      "Epoch 75/150\n",
      "16000/16000 [==============================] - 11s 710us/step - loss: 0.5527 - val_loss: 0.5945\n",
      "Epoch 76/150\n",
      "16000/16000 [==============================] - 11s 702us/step - loss: 0.5516 - val_loss: 0.5943\n",
      "Epoch 77/150\n",
      "16000/16000 [==============================] - 11s 711us/step - loss: 0.5506 - val_loss: 0.5942\n",
      "Epoch 78/150\n",
      "16000/16000 [==============================] - 11s 705us/step - loss: 0.5497 - val_loss: 0.5941\n",
      "Epoch 79/150\n",
      "16000/16000 [==============================] - 11s 702us/step - loss: 0.5483 - val_loss: 0.5936\n",
      "Epoch 80/150\n",
      "16000/16000 [==============================] - 11s 703us/step - loss: 0.5476 - val_loss: 0.5935\n",
      "Epoch 81/150\n",
      "16000/16000 [==============================] - 11s 702us/step - loss: 0.5464 - val_loss: 0.5930\n",
      "Epoch 82/150\n",
      "16000/16000 [==============================] - 11s 704us/step - loss: 0.5451 - val_loss: 0.5932\n",
      "Epoch 83/150\n",
      "16000/16000 [==============================] - 11s 697us/step - loss: 0.5443 - val_loss: 0.5929\n",
      "Epoch 84/150\n",
      "16000/16000 [==============================] - 11s 711us/step - loss: 0.5433 - val_loss: 0.5930\n",
      "Epoch 85/150\n",
      "16000/16000 [==============================] - 11s 706us/step - loss: 0.5421 - val_loss: 0.5926\n",
      "Epoch 86/150\n",
      "16000/16000 [==============================] - 11s 696us/step - loss: 0.5407 - val_loss: 0.5926\n",
      "Epoch 87/150\n",
      "16000/16000 [==============================] - 11s 710us/step - loss: 0.5402 - val_loss: 0.5926\n",
      "Epoch 88/150\n",
      "16000/16000 [==============================] - 11s 695us/step - loss: 0.5391 - val_loss: 0.5921\n",
      "Epoch 89/150\n",
      "16000/16000 [==============================] - 11s 706us/step - loss: 0.5380 - val_loss: 0.5921\n",
      "Epoch 90/150\n",
      "16000/16000 [==============================] - 11s 703us/step - loss: 0.5368 - val_loss: 0.5919\n",
      "Epoch 91/150\n",
      "16000/16000 [==============================] - 11s 700us/step - loss: 0.5363 - val_loss: 0.5916\n",
      "Epoch 92/150\n",
      "16000/16000 [==============================] - 11s 699us/step - loss: 0.5356 - val_loss: 0.5918\n",
      "Epoch 93/150\n",
      "16000/16000 [==============================] - 11s 701us/step - loss: 0.5343 - val_loss: 0.5914\n",
      "Epoch 94/150\n",
      "16000/16000 [==============================] - 11s 703us/step - loss: 0.5334 - val_loss: 0.5914\n",
      "Epoch 95/150\n",
      "16000/16000 [==============================] - 11s 703us/step - loss: 0.5326 - val_loss: 0.5910\n",
      "Epoch 96/150\n",
      "16000/16000 [==============================] - 11s 717us/step - loss: 0.5316 - val_loss: 0.5913\n",
      "Epoch 97/150\n",
      "16000/16000 [==============================] - 11s 696us/step - loss: 0.5306 - val_loss: 0.5911\n",
      "Epoch 98/150\n",
      "16000/16000 [==============================] - 11s 694us/step - loss: 0.5298 - val_loss: 0.5914\n",
      "Epoch 99/150\n",
      "16000/16000 [==============================] - 11s 699us/step - loss: 0.5290 - val_loss: 0.5918\n",
      "Epoch 100/150\n",
      "16000/16000 [==============================] - 11s 710us/step - loss: 0.5279 - val_loss: 0.5914\n",
      "Epoch 101/150\n",
      "16000/16000 [==============================] - 11s 704us/step - loss: 0.5276 - val_loss: 0.5909\n",
      "Epoch 102/150\n",
      "16000/16000 [==============================] - 11s 706us/step - loss: 0.5262 - val_loss: 0.5911\n",
      "Epoch 103/150\n",
      "16000/16000 [==============================] - 11s 707us/step - loss: 0.5258 - val_loss: 0.5909\n",
      "Epoch 104/150\n",
      "16000/16000 [==============================] - 11s 711us/step - loss: 0.5248 - val_loss: 0.5913\n",
      "Epoch 105/150\n",
      "16000/16000 [==============================] - 11s 701us/step - loss: 0.5240 - val_loss: 0.5914\n",
      "Epoch 106/150\n",
      "16000/16000 [==============================] - 11s 700us/step - loss: 0.5232 - val_loss: 0.5909\n",
      "Epoch 107/150\n",
      "16000/16000 [==============================] - 11s 708us/step - loss: 0.5220 - val_loss: 0.5909\n",
      "Epoch 108/150\n",
      "16000/16000 [==============================] - 11s 701us/step - loss: 0.5211 - val_loss: 0.5907\n",
      "Epoch 109/150\n",
      "16000/16000 [==============================] - 11s 696us/step - loss: 0.5204 - val_loss: 0.5908\n",
      "Epoch 110/150\n",
      "16000/16000 [==============================] - 11s 693us/step - loss: 0.5197 - val_loss: 0.5908\n",
      "Epoch 111/150\n",
      "16000/16000 [==============================] - 11s 702us/step - loss: 0.5188 - val_loss: 0.5909\n",
      "Epoch 112/150\n",
      "16000/16000 [==============================] - 11s 714us/step - loss: 0.5179 - val_loss: 0.5907\n",
      "Epoch 113/150\n",
      "16000/16000 [==============================] - 11s 706us/step - loss: 0.5171 - val_loss: 0.5904\n",
      "Epoch 114/150\n",
      "16000/16000 [==============================] - 11s 700us/step - loss: 0.5159 - val_loss: 0.5908\n",
      "Epoch 115/150\n",
      "16000/16000 [==============================] - 11s 698us/step - loss: 0.5153 - val_loss: 0.5907\n",
      "Epoch 116/150\n",
      "16000/16000 [==============================] - 11s 711us/step - loss: 0.5145 - val_loss: 0.5906\n",
      "Epoch 117/150\n",
      "16000/16000 [==============================] - 11s 702us/step - loss: 0.5136 - val_loss: 0.5907\n",
      "Epoch 118/150\n",
      "16000/16000 [==============================] - 11s 699us/step - loss: 0.5129 - val_loss: 0.5904\n",
      "Epoch 119/150\n",
      "16000/16000 [==============================] - 12s 720us/step - loss: 0.5119 - val_loss: 0.5908\n",
      "Epoch 120/150\n",
      "16000/16000 [==============================] - 11s 716us/step - loss: 0.5109 - val_loss: 0.5910\n",
      "Epoch 121/150\n",
      "16000/16000 [==============================] - 11s 713us/step - loss: 0.5105 - val_loss: 0.5910\n",
      "Epoch 122/150\n",
      "16000/16000 [==============================] - 11s 713us/step - loss: 0.5099 - val_loss: 0.5908\n",
      "Epoch 123/150\n",
      "16000/16000 [==============================] - 11s 705us/step - loss: 0.5085 - val_loss: 0.5908\n",
      "Epoch 124/150\n",
      "16000/16000 [==============================] - 11s 713us/step - loss: 0.5079 - val_loss: 0.5911\n",
      "Epoch 125/150\n",
      "16000/16000 [==============================] - 11s 700us/step - loss: 0.5073 - val_loss: 0.5907\n",
      "Epoch 126/150\n",
      "16000/16000 [==============================] - 11s 696us/step - loss: 0.5068 - val_loss: 0.5914\n",
      "Epoch 127/150\n",
      "16000/16000 [==============================] - 11s 705us/step - loss: 0.5057 - val_loss: 0.5908\n",
      "Epoch 128/150\n",
      "16000/16000 [==============================] - 11s 709us/step - loss: 0.5052 - val_loss: 0.5912\n",
      "Epoch 129/150\n",
      "16000/16000 [==============================] - 11s 709us/step - loss: 0.5045 - val_loss: 0.5910\n",
      "Epoch 130/150\n",
      "16000/16000 [==============================] - 11s 708us/step - loss: 0.5038 - val_loss: 0.5911\n",
      "Epoch 131/150\n",
      "16000/16000 [==============================] - 11s 718us/step - loss: 0.5027 - val_loss: 0.5910\n",
      "Epoch 132/150\n",
      "16000/16000 [==============================] - 11s 704us/step - loss: 0.5017 - val_loss: 0.5915\n",
      "Epoch 133/150\n",
      "16000/16000 [==============================] - 11s 710us/step - loss: 0.5013 - val_loss: 0.5916\n",
      "Epoch 134/150\n",
      "16000/16000 [==============================] - 11s 705us/step - loss: 0.5010 - val_loss: 0.5914\n",
      "Epoch 135/150\n",
      "16000/16000 [==============================] - 11s 713us/step - loss: 0.5000 - val_loss: 0.5914\n",
      "Epoch 136/150\n",
      "16000/16000 [==============================] - 11s 713us/step - loss: 0.4991 - val_loss: 0.5921\n",
      "Epoch 137/150\n",
      "16000/16000 [==============================] - 11s 716us/step - loss: 0.4986 - val_loss: 0.5921\n",
      "Epoch 138/150\n",
      "16000/16000 [==============================] - 11s 701us/step - loss: 0.4984 - val_loss: 0.5921\n",
      "Epoch 139/150\n",
      "16000/16000 [==============================] - 11s 711us/step - loss: 0.4978 - val_loss: 0.5920\n",
      "Epoch 140/150\n",
      "16000/16000 [==============================] - 11s 698us/step - loss: 0.4968 - val_loss: 0.5927\n",
      "Epoch 141/150\n",
      "16000/16000 [==============================] - 11s 708us/step - loss: 0.4966 - val_loss: 0.5920\n",
      "Epoch 142/150\n",
      "16000/16000 [==============================] - 12s 721us/step - loss: 0.4954 - val_loss: 0.5923\n",
      "Epoch 143/150\n",
      "16000/16000 [==============================] - 11s 704us/step - loss: 0.4949 - val_loss: 0.5926\n",
      "Epoch 144/150\n",
      "16000/16000 [==============================] - 11s 711us/step - loss: 0.4939 - val_loss: 0.5928\n",
      "Epoch 145/150\n",
      "16000/16000 [==============================] - 11s 704us/step - loss: 0.4931 - val_loss: 0.5920\n",
      "Epoch 146/150\n",
      "16000/16000 [==============================] - 11s 703us/step - loss: 0.4923 - val_loss: 0.5929\n",
      "Epoch 147/150\n",
      "16000/16000 [==============================] - 11s 711us/step - loss: 0.4918 - val_loss: 0.5925\n",
      "Epoch 148/150\n",
      "16000/16000 [==============================] - 11s 715us/step - loss: 0.4912 - val_loss: 0.5924\n",
      "Epoch 149/150\n",
      "16000/16000 [==============================] - 11s 709us/step - loss: 0.4905 - val_loss: 0.5921\n",
      "Epoch 150/150\n",
      "16000/16000 [==============================] - 11s 705us/step - loss: 0.4899 - val_loss: 0.5923\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras import layers\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def lenet_model():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(6696, input_shape=(300,), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3348, activation = 'linear'))\n",
    "    model.compile(Adam(lr = .01), loss = 'mse')\n",
    "    return model\n",
    "\n",
    "model = lenet_model()\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(x_train,y_train, epochs = 150, validation_data = (x_dev,y_dev), shuffle = 1, batch_size = 5000, verbose = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.ylim(.45, .8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Test the NN model on our development data, computing its MAP@20 with cosine distance, and investigating the quality of the rankings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-1-2-3-4-5-6-7-8-9-10-11-12-13-14-15-16-17-18-19-20-21-22-23-24-25-26-27-28-29-30-31-32-33-34-35-36-37-38-39-40-41-42-43-44-45-46-47-48-49-50-51-52-53-54-55-56-57-58-59-60-61-62-63-64-65-66-67-68-69-70-71-72-73-74-75-76-77-78-79-80-81-82-83-84-85-86-87-88-89-90-91-92-93-94-95-96-97-98-99-100-101-102-103-104-105-106-107-108-109-110-111-112-113-114-115-116-117-118-119-120-121-122-123-124-125-126-127-128-129-130-131-132-133-134-135-136-137-138-139-140-141-142-143-144-145-146-147-148-149-150-151-152-153-154-155-156-157-158-159-160-161-162-163-164-165-166-167-168-169-170-171-172-173-174-175-176-177-178-179-180-181-182-183-184-185-186-187-188-189-190-191-192-193-194-195-196-197-198-199-200-201-202-203-204-205-206-207-208-209-210-211-212-213-214-215-216-217-218-219-220-221-222-223-224-225-226-227-228-229-230-231-232-233-234-235-236-237-238-239-240-241-242-243-244-245-246-247-248-249-250-251-252-253-254-255-256-257-258-259-260-261-262-263-264-265-266-267-268-269-270-271-272-273-274-275-276-277-278-279-280-281-282-283-284-285-286-287-288-289-290-291-292-293-294-295-296-297-298-299-300-301-302-303-304-305-306-307-308-309-310-311-312-313-314-315-316-317-318-319-320-321-322-323-324-325-326-327-328-329-330-331-332-333-334-335-336-337-338-339-340-341-342-343-344-345-346-347-348-349-350-351-352-353-354-355-356-357-358-359-360-361-362-363-364-365-366-367-368-369-370-371-372-373-374-375-376-377-378-379-380-381-382-383-384-385-386-387-388-389-390-391-392-393-394-395-396-397-398-399-400-401-402-403-404-405-406-407-408-409-410-411-412-413-414-415-416-417-418-419-420-421-422-423-424-425-426-427-428-429-430-431-432-433-434-435-436-437-438-439-440-441-442-443-444-445-446-447-448-449-450-451-452-453-454-455-456-457-458-459-460-461-462-463-464-465-466-467-468-469-470-471-472-473-474-475-476-477-478-479-480-481-482-483-484-485-486-487-488-489-490-491-492-493-494-495-496-497-498-499-500-501-502-503-504-505-506-507-508-509-510-511-512-513-514-515-516-517-518-519-520-521-522-523-524-525-526-527-528-529-530-531-532-533-534-535-536-537-538-539-540-541-542-543-544-545-546-547-548-549-550-551-552-553-554-555-556-557-558-559-560-561-562-563-564-565-566-567-568-569-570-571-572-573-574-575-576-577-578-579-580-581-582-583-584-585-586-587-588-589-590-591-592-593-594-595-596-597-598-599-600-601-602-603-604-605-606-607-608-609-610-611-612-613-614-615-616-617-618-619-620-621-622-623-624-625-626-627-628-629-630-631-632-633-634-635-636-637-638-639-640-641-642-643-644-645-646-647-648-649-650-651-652-653-654-655-656-657-658-659-660-661-662-663-664-665-666-667-668-669-670-671-672-673-674-675-676-677-678-679-680-681-682-683-684-685-686-687-688-689-690-691-692-693-694-695-696-697-698-699-700-701-702-703-704-705-706-707-708-709-710-711-712-713-714-715-716-717-718-719-720-721-722-723-724-725-726-727-728-729-730-731-732-733-734-735-736-737-738-739-740-741-742-743-744-745-746-747-748-749-750-751-752-753-754-755-756-757-758-759-760-761-762-763-764-765-766-767-768-769-770-771-772-773-774-775-776-777-778-779-780-781-782-783-784-785-786-787-788-789-790-791-792-793-794-795-796-797-798-799-800-801-802-803-804-805-806-807-808-809-810-811-812-813-814-815-816-817-818-819-820-821-822-823-824-825-826-827-828-829-830-831-832-833-834-835-836-837-838-839-840-841-842-843-844-845-846-847-848-849-850-851-852-853-854-855-856-857-858-859-860-861-862-863-864-865-866-867-868-869-870-871-872-873-874-875-876-877-878-879-880-881-882-883-884-885-886-887-888-889-890-891-892-893-894-895-896-897-898-899-900-901-902-903-904-905-906-907-908-909-910-911-912-913-914-915-916-917-918-919-920-921-922-923-924-925-926-927-928-929-930-931-932-933-934-935-936-937-938-939-940-941-942-943-944-945-946-947-948-949-950-951-952-953-954-955-956-957-958-959-960-961-962-963-964-965-966-967-968-969-970-971-972-973-974-975-976-977-978-979-980-981-982-983-984-985-986-987-988-989-990-991-992-993-994-995-996-997-998-999-1000-1001-1002-1003-1004-1005-1006-1007-1008-1009-1010-1011-1012-1013-1014-1015-1016-1017-1018-1019-1020-1021-1022-1023-1024-1025-1026-1027-1028-1029-1030-1031-1032-1033-1034-1035-1036-1037-1038-1039-1040-1041-1042-1043-1044-1045-1046-1047-1048-1049-1050-1051-1052-1053-1054-1055-1056-1057-1058-1059-1060-1061-1062-1063-1064-1065-1066-1067-1068-1069-1070-1071-1072-1073-1074-1075-1076-1077-1078-1079-1080-1081-1082-1083-1084-1085-1086-1087-1088-1089-1090-1091-1092-1093-1094-1095-1096-1097-1098-1099-1100-1101-1102-1103-1104-1105-1106-1107-1108-1109-1110-1111-1112-1113-1114-1115-1116-1117-1118-1119-1120-1121-1122-1123-1124-1125-1126-1127-1128-1129-1130-1131-1132-1133-1134-1135-1136-1137-1138-1139-1140-1141-1142-1143-1144-1145-1146-1147-1148-1149-1150-1151-1152-1153-1154-1155-1156-1157-1158-1159-1160-1161-1162-1163-1164-1165-1166-1167-1168-1169-1170-1171-1172-1173-1174-1175-1176-1177-1178-1179-1180-1181-1182-1183-1184-1185-1186-1187-1188-1189-1190-1191-1192-1193-1194-1195-1196-1197-1198-1199-1200-1201-1202-1203-1204-1205-1206-1207-1208-1209-1210-1211-1212-1213-1214-1215-1216-1217-1218-1219-1220-1221-1222-1223-1224-1225-1226-1227-1228-1229-1230-1231-1232-1233-1234-1235-1236-1237-1238-1239-1240-1241-1242-1243-1244-1245-1246-1247-1248-1249-1250-1251-1252-1253-1254-1255-1256-1257-1258-1259-1260-1261-1262-1263-1264-1265-1266-1267-1268-1269-1270-1271-1272-1273-1274-1275-1276-1277-1278-1279-1280-1281-1282-1283-1284-1285-1286-1287-1288-1289-1290-1291-1292-1293-1294-1295-1296-1297-1298-1299-1300-1301-1302-1303-1304-1305-1306-1307-1308-1309-1310-1311-1312-1313-1314-1315-1316-1317-1318-1319-1320-1321-1322-1323-1324-1325-1326-1327-1328-1329-1330-1331-1332-1333-1334-1335-1336-1337-1338-1339-1340-1341-1342-1343-1344-1345-1346-1347-1348-1349-1350-1351-1352-1353-1354-1355-1356-1357-1358-1359-1360-1361-1362-1363-1364-1365-1366-1367-1368-1369-1370-1371-1372-1373-1374-1375-1376-1377-1378-1379-1380-1381-1382-1383-1384-1385-1386-1387-1388-1389-1390-1391-1392-1393-1394-1395-1396-1397-1398-1399-1400-1401-1402-1403-1404-1405-1406-1407-1408-1409-1410-1411-1412-1413-1414-1415-1416-1417-1418-1419-1420-1421-1422-1423-1424-1425-1426-1427-1428-1429-1430-1431-1432-1433-1434-1435-1436-1437-1438-1439-1440-1441-1442-1443-1444-1445-1446-1447-1448-1449-1450-1451-1452-1453-1454-1455-1456-1457-1458-1459-1460-1461-1462-1463-1464-1465-1466-1467-1468-1469-1470-1471-1472-1473-1474-1475-1476-1477-1478-1479-1480-1481-1482-1483-1484-1485-1486-1487-1488-1489-1490-1491-1492-1493-1494-1495-1496-1497-1498-1499-1500-1501-1502-1503-1504-1505-1506-1507-1508-1509-1510-1511-1512-1513-1514-1515-1516-1517-1518-1519-1520-1521-1522-1523-1524-1525-1526-1527-1528-1529-1530-1531-1532-1533-1534-1535-1536-1537-1538-1539-1540-1541-1542-1543-1544-1545-1546-1547-1548-1549-1550-1551-1552-1553-1554-1555-1556-1557-1558-1559-1560-1561-1562-1563-1564-1565-1566-1567-1568-1569-1570-1571-1572-1573-1574-1575-1576-1577-1578-1579-1580-1581-1582-1583-1584-1585-1586-1587-1588-1589-1590-1591-1592-1593-1594-1595-1596-1597-1598-1599-1600-1601-1602-1603-1604-1605-1606-1607-1608-1609-1610-1611-1612-1613-1614-1615-1616-1617-1618-1619-1620-1621-1622-1623-1624-1625-1626-1627-1628-1629-1630-1631-1632-1633-1634-1635-1636-1637-1638-1639-1640-1641-1642-1643-1644-1645-1646-1647-1648-1649-1650-1651-1652-1653-1654-1655-1656-1657-1658-1659-1660-1661-1662-1663-1664-1665-1666-1667-1668-1669-1670-1671-1672-1673-1674-1675-1676-1677-1678-1679-1680-1681-1682-1683-1684-1685-1686-1687-1688-1689-1690-1691-1692-1693-1694-1695-1696-1697-1698-1699-1700-1701-1702-1703-1704-1705-1706-1707-1708-1709-1710-1711-1712-1713-1714-1715-1716-1717-1718-1719-1720-1721-1722-1723-1724-1725-1726-1727-1728-1729-1730-1731-1732-1733-1734-1735-1736-1737-1738-1739-1740-1741-1742-1743-1744-1745-1746-1747-1748-1749-1750-1751-1752-1753-1754-1755-1756-1757-1758-1759-1760-1761-1762-1763-1764-1765-1766-1767-1768-1769-1770-1771-1772-1773-1774-1775-1776-1777-1778-1779-1780-1781-1782-1783-1784-1785-1786-1787-1788-1789-1790-1791-1792-1793-1794-1795-1796-1797-1798-1799-1800-1801-1802-1803-1804-1805-1806-1807-1808-1809-1810-1811-1812-1813-1814-1815-1816-1817-1818-1819-1820-1821-1822-1823-1824-1825-1826-1827-1828-1829-1830-1831-1832-1833-1834-1835-1836-1837-1838-1839-1840-1841-1842-1843-1844-1845-1846-1847-1848-1849-1850-1851-1852-1853-1854-1855-1856-1857-1858-1859-1860-1861-1862-1863-1864-1865-1866-1867-1868-1869-1870-1871-1872-1873-1874-1875-1876-1877-1878-1879-1880-1881-1882-1883-1884-1885-1886-1887-1888-1889-1890-1891-1892-1893-1894-1895-1896-1897-1898-1899-1900-1901-1902-1903-1904-1905-1906-1907-1908-1909-1910-1911-1912-1913-1914-1915-1916-1917-1918-1919-1920-1921-1922-1923-1924-1925-1926-1927-1928-1929-1930-1931-1932-1933-1934-1935-1936-1937-1938-1939-1940-1941-1942-1943-1944-1945-1946-1947-1948-1949-1950-1951-1952-1953-1954-1955-1956-1957-1958-1959-1960-1961-1962-1963-1964-1965-1966-1967-1968-1969-1970-1971-1972-1973-1974-1975-1976-1977-1978-1979-1980-1981-1982-1983-1984-1985-1986-1987-1988-1989-1990-1991-1992-1993-1994-1995-1996-1997-1998-1999-Development MAP@20: 0.2872981364524708\n",
      "Mean index of true image 21.0275\n",
      "Median index of true image 6.5\n",
      "Shape:  (4000, 3348)\n",
      "Shape:  (4000, 3348)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import DistanceMetric\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.spatial.distance import minkowski\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy import spatial\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "def dist_matrix(x1, x2):\n",
    "    return ((np.expand_dims(x1, 1) - np.expand_dims(x2, 0)) ** 2).sum(2) ** 0.5\n",
    "\n",
    "def get_dif(Di,Ti):\n",
    "    return spatial.distance.cosine(Di, Ti)\n",
    "\n",
    "\n",
    "##NN\n",
    "y_dev_pred = model.predict(x_dev)\n",
    "\n",
    "\n",
    "ranks = []\n",
    "for i in range(2000):\n",
    "    print(i, end=\"-\")\n",
    "    tmp = np.zeros(2000)\n",
    "    for j in range(2000):\n",
    "        tmp[j] = get_dif(y_dev_pred[i], y_dev[j])\n",
    "        \n",
    "#     i_sorted = np.argsort(tmp)[::-1]\n",
    "    ranks.append(tmp)\n",
    "\n",
    "dev_distances = ranks\n",
    "\n",
    "\n",
    "dev_scores = []\n",
    "dev_pos_list = []\n",
    "\n",
    "for i in range(num_dev):\n",
    "    pred_dist_idx = list(np.argsort(dev_distances[i]))\n",
    "    dev_pos = pred_dist_idx.index(i)\n",
    "    dev_pos_list.append(dev_pos)\n",
    "    if dev_pos < 20:\n",
    "        dev_scores.append(1 / (dev_pos + 1))\n",
    "    else:\n",
    "        dev_scores.append(0.0)\n",
    "\n",
    "print(\"Development MAP@20:\", np.mean(dev_scores))\n",
    "print(\"Mean index of true image\", np.mean(dev_pos_list))\n",
    "print(\"Median index of true image\", np.median(dev_pos_list))\n",
    "print(\"Shape: \", y_dev_pred.shape)\n",
    "print(\"Shape: \", y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import DistanceMetric\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.spatial.distance import minkowski\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy import spatial\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def get_dif(Di,Ti):\n",
    "    return spatial.distance.cosine(Di, Ti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Finally we use our model to compute top-20 predictions on the test data that can be submitted to Kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_29 (Dense)             (None, 6696)              2015496   \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 6696)              0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 3348)              22421556  \n",
      "=================================================================\n",
      "Total params: 24,437,052\n",
      "Trainable params: 24,437,052\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "20000/20000 [==============================] - 13s 655us/step - loss: 25.6735\n",
      "Epoch 2/200\n",
      "20000/20000 [==============================] - 13s 638us/step - loss: 12.4426\n",
      "Epoch 3/200\n",
      "20000/20000 [==============================] - 13s 642us/step - loss: 5.8128\n",
      "Epoch 4/200\n",
      "20000/20000 [==============================] - 13s 641us/step - loss: 3.3843\n",
      "Epoch 5/200\n",
      "20000/20000 [==============================] - 13s 644us/step - loss: 1.8297\n",
      "Epoch 6/200\n",
      "20000/20000 [==============================] - 13s 639us/step - loss: 1.5760\n",
      "Epoch 7/200\n",
      "20000/20000 [==============================] - 13s 649us/step - loss: 1.3889\n",
      "Epoch 8/200\n",
      "20000/20000 [==============================] - 13s 640us/step - loss: 1.2149\n",
      "Epoch 9/200\n",
      "20000/20000 [==============================] - 13s 638us/step - loss: 1.0583\n",
      "Epoch 10/200\n",
      "20000/20000 [==============================] - 13s 648us/step - loss: 0.9395\n",
      "Epoch 11/200\n",
      "20000/20000 [==============================] - 13s 652us/step - loss: 0.8579\n",
      "Epoch 12/200\n",
      "20000/20000 [==============================] - 13s 642us/step - loss: 0.8006\n",
      "Epoch 13/200\n",
      "20000/20000 [==============================] - 13s 639us/step - loss: 0.7616\n",
      "Epoch 14/200\n",
      "20000/20000 [==============================] - 13s 641us/step - loss: 0.7359\n",
      "Epoch 15/200\n",
      "20000/20000 [==============================] - 13s 645us/step - loss: 0.7158\n",
      "Epoch 16/200\n",
      "20000/20000 [==============================] - 13s 644us/step - loss: 0.7001\n",
      "Epoch 17/200\n",
      "20000/20000 [==============================] - 13s 646us/step - loss: 0.6886\n",
      "Epoch 18/200\n",
      "20000/20000 [==============================] - 13s 646us/step - loss: 0.6790\n",
      "Epoch 19/200\n",
      "20000/20000 [==============================] - 13s 646us/step - loss: 0.6710\n",
      "Epoch 20/200\n",
      "20000/20000 [==============================] - 13s 654us/step - loss: 0.6645\n",
      "Epoch 21/200\n",
      "20000/20000 [==============================] - 13s 646us/step - loss: 0.6590\n",
      "Epoch 22/200\n",
      "20000/20000 [==============================] - 13s 648us/step - loss: 0.6536\n",
      "Epoch 23/200\n",
      "20000/20000 [==============================] - 13s 648us/step - loss: 0.6494\n",
      "Epoch 24/200\n",
      "20000/20000 [==============================] - 13s 650us/step - loss: 0.6450\n",
      "Epoch 25/200\n",
      "20000/20000 [==============================] - 13s 653us/step - loss: 0.6412\n",
      "Epoch 26/200\n",
      "20000/20000 [==============================] - 13s 645us/step - loss: 0.6377\n",
      "Epoch 27/200\n",
      "20000/20000 [==============================] - 13s 653us/step - loss: 0.6344\n",
      "Epoch 28/200\n",
      "20000/20000 [==============================] - 13s 648us/step - loss: 0.6312\n",
      "Epoch 29/200\n",
      "20000/20000 [==============================] - 13s 648us/step - loss: 0.6285\n",
      "Epoch 30/200\n",
      "20000/20000 [==============================] - 13s 649us/step - loss: 0.6259\n",
      "Epoch 31/200\n",
      "20000/20000 [==============================] - 13s 645us/step - loss: 0.6233\n",
      "Epoch 32/200\n",
      "20000/20000 [==============================] - 13s 640us/step - loss: 0.6207\n",
      "Epoch 33/200\n",
      "20000/20000 [==============================] - 13s 647us/step - loss: 0.6186\n",
      "Epoch 34/200\n",
      "20000/20000 [==============================] - 13s 637us/step - loss: 0.6163\n",
      "Epoch 35/200\n",
      "20000/20000 [==============================] - 13s 647us/step - loss: 0.6138\n",
      "Epoch 36/200\n",
      "20000/20000 [==============================] - 13s 646us/step - loss: 0.6118\n",
      "Epoch 37/200\n",
      "20000/20000 [==============================] - 13s 651us/step - loss: 0.6100\n",
      "Epoch 38/200\n",
      "20000/20000 [==============================] - 13s 641us/step - loss: 0.6079\n",
      "Epoch 39/200\n",
      "20000/20000 [==============================] - 13s 648us/step - loss: 0.6062\n",
      "Epoch 40/200\n",
      "20000/20000 [==============================] - 13s 636us/step - loss: 0.6041\n",
      "Epoch 41/200\n",
      "20000/20000 [==============================] - 13s 657us/step - loss: 0.6023\n",
      "Epoch 42/200\n",
      "20000/20000 [==============================] - 13s 642us/step - loss: 0.6007\n",
      "Epoch 43/200\n",
      "20000/20000 [==============================] - 13s 646us/step - loss: 0.5990\n",
      "Epoch 44/200\n",
      "20000/20000 [==============================] - 13s 650us/step - loss: 0.5974\n",
      "Epoch 45/200\n",
      "20000/20000 [==============================] - 13s 646us/step - loss: 0.5958\n",
      "Epoch 46/200\n",
      "20000/20000 [==============================] - 13s 643us/step - loss: 0.5943\n",
      "Epoch 47/200\n",
      "20000/20000 [==============================] - 13s 645us/step - loss: 0.5927\n",
      "Epoch 48/200\n",
      "20000/20000 [==============================] - 13s 649us/step - loss: 0.5913\n",
      "Epoch 49/200\n",
      "20000/20000 [==============================] - 13s 649us/step - loss: 0.5898\n",
      "Epoch 50/200\n",
      "20000/20000 [==============================] - 13s 644us/step - loss: 0.5884\n",
      "Epoch 51/200\n",
      "20000/20000 [==============================] - 13s 648us/step - loss: 0.5868\n",
      "Epoch 52/200\n",
      "20000/20000 [==============================] - 13s 644us/step - loss: 0.5854\n",
      "Epoch 53/200\n",
      "20000/20000 [==============================] - 13s 647us/step - loss: 0.5841\n",
      "Epoch 54/200\n",
      "20000/20000 [==============================] - 13s 636us/step - loss: 0.5826\n",
      "Epoch 55/200\n",
      "20000/20000 [==============================] - 13s 645us/step - loss: 0.5814\n",
      "Epoch 56/200\n",
      "20000/20000 [==============================] - 13s 646us/step - loss: 0.5803\n",
      "Epoch 57/200\n",
      "20000/20000 [==============================] - 13s 644us/step - loss: 0.5789\n",
      "Epoch 58/200\n",
      "20000/20000 [==============================] - 13s 641us/step - loss: 0.5778\n",
      "Epoch 59/200\n",
      "20000/20000 [==============================] - 13s 643us/step - loss: 0.5766\n",
      "Epoch 60/200\n",
      "20000/20000 [==============================] - 13s 643us/step - loss: 0.5754\n",
      "Epoch 61/200\n",
      "20000/20000 [==============================] - 13s 654us/step - loss: 0.5738\n",
      "Epoch 62/200\n",
      "20000/20000 [==============================] - 13s 643us/step - loss: 0.5726\n",
      "Epoch 63/200\n",
      "20000/20000 [==============================] - 13s 649us/step - loss: 0.5718\n",
      "Epoch 64/200\n",
      "20000/20000 [==============================] - 13s 647us/step - loss: 0.5706\n",
      "Epoch 65/200\n",
      "20000/20000 [==============================] - 13s 646us/step - loss: 0.5694\n",
      "Epoch 66/200\n",
      "20000/20000 [==============================] - 13s 643us/step - loss: 0.5685\n",
      "Epoch 67/200\n",
      "20000/20000 [==============================] - 13s 642us/step - loss: 0.5673\n",
      "Epoch 68/200\n",
      "20000/20000 [==============================] - 13s 645us/step - loss: 0.5662\n",
      "Epoch 69/200\n",
      "20000/20000 [==============================] - 13s 640us/step - loss: 0.5650\n",
      "Epoch 70/200\n",
      "20000/20000 [==============================] - 13s 646us/step - loss: 0.5642\n",
      "Epoch 71/200\n",
      "20000/20000 [==============================] - 13s 648us/step - loss: 0.5629\n",
      "Epoch 72/200\n",
      "20000/20000 [==============================] - 13s 644us/step - loss: 0.5618\n",
      "Epoch 73/200\n",
      "20000/20000 [==============================] - 13s 643us/step - loss: 0.5608\n",
      "Epoch 74/200\n",
      "20000/20000 [==============================] - 13s 644us/step - loss: 0.5597\n",
      "Epoch 75/200\n",
      "20000/20000 [==============================] - 13s 643us/step - loss: 0.5587\n",
      "Epoch 76/200\n",
      "20000/20000 [==============================] - 13s 642us/step - loss: 0.5579\n",
      "Epoch 77/200\n",
      "20000/20000 [==============================] - 13s 643us/step - loss: 0.5567\n",
      "Epoch 78/200\n",
      "20000/20000 [==============================] - 13s 646us/step - loss: 0.5559\n",
      "Epoch 79/200\n",
      "20000/20000 [==============================] - 13s 640us/step - loss: 0.5549\n",
      "Epoch 80/200\n",
      "20000/20000 [==============================] - 13s 642us/step - loss: 0.5540\n",
      "Epoch 81/200\n",
      "20000/20000 [==============================] - 13s 649us/step - loss: 0.5527\n",
      "Epoch 82/200\n",
      "20000/20000 [==============================] - 13s 648us/step - loss: 0.5520\n",
      "Epoch 83/200\n",
      "20000/20000 [==============================] - 13s 647us/step - loss: 0.5509\n",
      "Epoch 84/200\n",
      "20000/20000 [==============================] - 13s 647us/step - loss: 0.5501\n",
      "Epoch 85/200\n",
      "20000/20000 [==============================] - 13s 644us/step - loss: 0.5491\n",
      "Epoch 86/200\n",
      "20000/20000 [==============================] - 13s 645us/step - loss: 0.5482\n",
      "Epoch 87/200\n",
      "20000/20000 [==============================] - 13s 643us/step - loss: 0.5475\n",
      "Epoch 88/200\n",
      "20000/20000 [==============================] - 13s 649us/step - loss: 0.5464\n",
      "Epoch 89/200\n",
      "20000/20000 [==============================] - 13s 643us/step - loss: 0.5456\n",
      "Epoch 90/200\n",
      "20000/20000 [==============================] - 13s 649us/step - loss: 0.5446\n",
      "Epoch 91/200\n",
      "20000/20000 [==============================] - 13s 653us/step - loss: 0.5437\n",
      "Epoch 92/200\n",
      "20000/20000 [==============================] - 13s 644us/step - loss: 0.5429\n",
      "Epoch 93/200\n",
      "20000/20000 [==============================] - 13s 643us/step - loss: 0.5425\n",
      "Epoch 94/200\n",
      "20000/20000 [==============================] - 13s 647us/step - loss: 0.5414\n",
      "Epoch 95/200\n",
      "20000/20000 [==============================] - 13s 655us/step - loss: 0.5404\n",
      "Epoch 96/200\n",
      "20000/20000 [==============================] - 13s 652us/step - loss: 0.5394\n",
      "Epoch 97/200\n",
      "20000/20000 [==============================] - 13s 650us/step - loss: 0.5389\n",
      "Epoch 98/200\n",
      "20000/20000 [==============================] - 13s 642us/step - loss: 0.5379\n",
      "Epoch 99/200\n",
      "20000/20000 [==============================] - 13s 644us/step - loss: 0.5371\n",
      "Epoch 100/200\n",
      "20000/20000 [==============================] - 13s 659us/step - loss: 0.5360\n",
      "Epoch 101/200\n",
      "20000/20000 [==============================] - 13s 652us/step - loss: 0.5355\n",
      "Epoch 102/200\n",
      "20000/20000 [==============================] - 13s 643us/step - loss: 0.5343\n",
      "Epoch 103/200\n",
      "20000/20000 [==============================] - 13s 640us/step - loss: 0.5336\n",
      "Epoch 104/200\n",
      "20000/20000 [==============================] - 13s 639us/step - loss: 0.5331\n",
      "Epoch 105/200\n",
      "20000/20000 [==============================] - 13s 645us/step - loss: 0.5319\n",
      "Epoch 106/200\n",
      "20000/20000 [==============================] - 13s 649us/step - loss: 0.5311\n",
      "Epoch 107/200\n",
      "20000/20000 [==============================] - 13s 644us/step - loss: 0.5304\n",
      "Epoch 108/200\n",
      "20000/20000 [==============================] - 13s 643us/step - loss: 0.5294\n",
      "Epoch 109/200\n",
      "20000/20000 [==============================] - 13s 643us/step - loss: 0.5286\n",
      "Epoch 110/200\n",
      "20000/20000 [==============================] - 13s 639us/step - loss: 0.5280\n",
      "Epoch 111/200\n",
      "20000/20000 [==============================] - 13s 641us/step - loss: 0.5270\n",
      "Epoch 112/200\n",
      "20000/20000 [==============================] - 13s 646us/step - loss: 0.5265\n",
      "Epoch 113/200\n",
      "20000/20000 [==============================] - 13s 641us/step - loss: 0.5256\n",
      "Epoch 114/200\n",
      "20000/20000 [==============================] - 13s 645us/step - loss: 0.5248\n",
      "Epoch 115/200\n",
      "20000/20000 [==============================] - 13s 639us/step - loss: 0.5240\n",
      "Epoch 116/200\n",
      "20000/20000 [==============================] - 13s 646us/step - loss: 0.5235\n",
      "Epoch 117/200\n",
      "20000/20000 [==============================] - 13s 647us/step - loss: 0.5227\n",
      "Epoch 118/200\n",
      "20000/20000 [==============================] - 13s 643us/step - loss: 0.5220\n",
      "Epoch 119/200\n",
      "20000/20000 [==============================] - 13s 638us/step - loss: 0.5211\n",
      "Epoch 120/200\n",
      "20000/20000 [==============================] - 13s 641us/step - loss: 0.5203\n",
      "Epoch 121/200\n",
      "20000/20000 [==============================] - 13s 642us/step - loss: 0.5195\n",
      "Epoch 122/200\n",
      "20000/20000 [==============================] - 13s 649us/step - loss: 0.5189\n",
      "Epoch 123/200\n",
      "20000/20000 [==============================] - 13s 644us/step - loss: 0.5183\n",
      "Epoch 124/200\n",
      "20000/20000 [==============================] - 13s 641us/step - loss: 0.5176\n",
      "Epoch 125/200\n",
      "20000/20000 [==============================] - 13s 636us/step - loss: 0.5169\n",
      "Epoch 126/200\n",
      "20000/20000 [==============================] - 13s 646us/step - loss: 0.5160\n",
      "Epoch 127/200\n",
      "20000/20000 [==============================] - 13s 646us/step - loss: 0.5153\n",
      "Epoch 128/200\n",
      "20000/20000 [==============================] - 13s 651us/step - loss: 0.5148\n",
      "Epoch 129/200\n",
      "20000/20000 [==============================] - 13s 644us/step - loss: 0.5140\n",
      "Epoch 130/200\n",
      "20000/20000 [==============================] - 13s 642us/step - loss: 0.5131\n",
      "Epoch 131/200\n",
      "20000/20000 [==============================] - 13s 642us/step - loss: 0.5122\n",
      "Epoch 132/200\n",
      "20000/20000 [==============================] - 13s 642us/step - loss: 0.5116\n",
      "Epoch 133/200\n",
      "20000/20000 [==============================] - 13s 646us/step - loss: 0.5110\n",
      "Epoch 134/200\n",
      "20000/20000 [==============================] - 13s 645us/step - loss: 0.5103\n",
      "Epoch 135/200\n",
      "20000/20000 [==============================] - 13s 647us/step - loss: 0.5097\n",
      "Epoch 136/200\n",
      "20000/20000 [==============================] - 13s 646us/step - loss: 0.5088\n",
      "Epoch 137/200\n",
      "20000/20000 [==============================] - 13s 641us/step - loss: 0.5080\n",
      "Epoch 138/200\n",
      "20000/20000 [==============================] - 13s 647us/step - loss: 0.5075\n",
      "Epoch 139/200\n",
      "20000/20000 [==============================] - 13s 646us/step - loss: 0.5066\n",
      "Epoch 140/200\n",
      "20000/20000 [==============================] - 13s 648us/step - loss: 0.5063\n",
      "Epoch 141/200\n",
      "20000/20000 [==============================] - 13s 645us/step - loss: 0.5051\n",
      "Epoch 142/200\n",
      "20000/20000 [==============================] - 13s 647us/step - loss: 0.5045\n",
      "Epoch 143/200\n",
      "20000/20000 [==============================] - 13s 644us/step - loss: 0.5041\n",
      "Epoch 144/200\n",
      "20000/20000 [==============================] - 13s 648us/step - loss: 0.5033\n",
      "Epoch 145/200\n",
      "20000/20000 [==============================] - 13s 652us/step - loss: 0.5028\n",
      "Epoch 146/200\n",
      "20000/20000 [==============================] - 13s 645us/step - loss: 0.5021\n",
      "Epoch 147/200\n",
      "20000/20000 [==============================] - 13s 645us/step - loss: 0.5018\n",
      "Epoch 148/200\n",
      "20000/20000 [==============================] - 13s 640us/step - loss: 0.5009\n",
      "Epoch 149/200\n",
      "20000/20000 [==============================] - 13s 642us/step - loss: 0.5005\n",
      "Epoch 150/200\n",
      "20000/20000 [==============================] - 13s 643us/step - loss: 0.4998\n",
      "Epoch 151/200\n",
      "20000/20000 [==============================] - 13s 653us/step - loss: 0.4993\n",
      "Epoch 152/200\n",
      "20000/20000 [==============================] - 13s 655us/step - loss: 0.4981\n",
      "Epoch 153/200\n",
      "20000/20000 [==============================] - 13s 649us/step - loss: 0.4977\n",
      "Epoch 154/200\n",
      "20000/20000 [==============================] - 13s 644us/step - loss: 0.4973\n",
      "Epoch 155/200\n",
      "20000/20000 [==============================] - 13s 644us/step - loss: 0.4965\n",
      "Epoch 156/200\n",
      "20000/20000 [==============================] - 13s 647us/step - loss: 0.4957\n",
      "Epoch 157/200\n",
      "20000/20000 [==============================] - 13s 647us/step - loss: 0.4954\n",
      "Epoch 158/200\n",
      "20000/20000 [==============================] - 13s 642us/step - loss: 0.4942\n",
      "Epoch 159/200\n",
      "20000/20000 [==============================] - 13s 647us/step - loss: 0.4937\n",
      "Epoch 160/200\n",
      "20000/20000 [==============================] - 13s 645us/step - loss: 0.4933\n",
      "Epoch 161/200\n",
      "20000/20000 [==============================] - 13s 643us/step - loss: 0.4927\n",
      "Epoch 162/200\n",
      "20000/20000 [==============================] - 13s 644us/step - loss: 0.4923\n",
      "Epoch 163/200\n",
      "20000/20000 [==============================] - 13s 645us/step - loss: 0.4914\n",
      "Epoch 164/200\n",
      "20000/20000 [==============================] - 13s 647us/step - loss: 0.4908\n",
      "Epoch 165/200\n",
      "20000/20000 [==============================] - 13s 649us/step - loss: 0.4903\n",
      "Epoch 166/200\n",
      "20000/20000 [==============================] - 13s 644us/step - loss: 0.4900\n",
      "Epoch 167/200\n",
      "20000/20000 [==============================] - 13s 642us/step - loss: 0.4896\n",
      "Epoch 168/200\n",
      "20000/20000 [==============================] - 13s 651us/step - loss: 0.4888\n",
      "Epoch 169/200\n",
      "20000/20000 [==============================] - 13s 649us/step - loss: 0.4877\n",
      "Epoch 170/200\n",
      "20000/20000 [==============================] - 13s 646us/step - loss: 0.4877\n",
      "Epoch 171/200\n",
      "20000/20000 [==============================] - 13s 654us/step - loss: 0.4875\n",
      "Epoch 172/200\n",
      "20000/20000 [==============================] - 13s 641us/step - loss: 0.4866\n",
      "Epoch 173/200\n",
      "20000/20000 [==============================] - 13s 641us/step - loss: 0.4863\n",
      "Epoch 174/200\n",
      "20000/20000 [==============================] - 13s 651us/step - loss: 0.4857\n",
      "Epoch 175/200\n",
      "20000/20000 [==============================] - 13s 652us/step - loss: 0.4844\n",
      "Epoch 176/200\n",
      "20000/20000 [==============================] - 13s 648us/step - loss: 0.4841\n",
      "Epoch 177/200\n",
      "20000/20000 [==============================] - 13s 643us/step - loss: 0.4834\n",
      "Epoch 178/200\n",
      "20000/20000 [==============================] - 13s 648us/step - loss: 0.4830\n",
      "Epoch 179/200\n",
      "20000/20000 [==============================] - 13s 641us/step - loss: 0.4827\n",
      "Epoch 180/200\n",
      "20000/20000 [==============================] - 13s 644us/step - loss: 0.4819\n",
      "Epoch 181/200\n",
      "20000/20000 [==============================] - 13s 646us/step - loss: 0.4811\n",
      "Epoch 182/200\n",
      "20000/20000 [==============================] - 13s 646us/step - loss: 0.4807\n",
      "Epoch 183/200\n",
      "20000/20000 [==============================] - 13s 645us/step - loss: 0.4803\n",
      "Epoch 184/200\n",
      "20000/20000 [==============================] - 13s 650us/step - loss: 0.4801\n",
      "Epoch 185/200\n",
      "20000/20000 [==============================] - 13s 641us/step - loss: 0.4793\n",
      "Epoch 186/200\n",
      "20000/20000 [==============================] - 13s 638us/step - loss: 0.4787\n",
      "Epoch 187/200\n",
      "20000/20000 [==============================] - 13s 641us/step - loss: 0.4779\n",
      "Epoch 188/200\n",
      "20000/20000 [==============================] - 13s 638us/step - loss: 0.4777\n",
      "Epoch 189/200\n",
      "20000/20000 [==============================] - 13s 643us/step - loss: 0.4769\n",
      "Epoch 190/200\n",
      "20000/20000 [==============================] - 13s 649us/step - loss: 0.4766\n",
      "Epoch 191/200\n",
      "20000/20000 [==============================] - 13s 644us/step - loss: 0.4762\n",
      "Epoch 192/200\n",
      "20000/20000 [==============================] - 13s 643us/step - loss: 0.4754\n",
      "Epoch 193/200\n",
      "20000/20000 [==============================] - 13s 650us/step - loss: 0.4751\n",
      "Epoch 194/200\n",
      "20000/20000 [==============================] - 13s 643us/step - loss: 0.4745\n",
      "Epoch 195/200\n",
      "20000/20000 [==============================] - 13s 641us/step - loss: 0.4741\n",
      "Epoch 196/200\n",
      "20000/20000 [==============================] - 13s 648us/step - loss: 0.4739\n",
      "Epoch 197/200\n",
      "20000/20000 [==============================] - 13s 650us/step - loss: 0.4734\n",
      "Epoch 198/200\n",
      "20000/20000 [==============================] - 13s 644us/step - loss: 0.4729\n",
      "Epoch 199/200\n",
      "20000/20000 [==============================] - 13s 647us/step - loss: 0.4725\n",
      "Epoch 200/200\n",
      "20000/20000 [==============================] - 13s 643us/step - loss: 0.4715\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "Output written!\n"
     ]
    }
   ],
   "source": [
    "# create test predictions\n",
    "x_train_all = np.concatenate([x_train, x_dev])\n",
    "y_train_all = np.concatenate([y_train, y_dev])\n",
    "# reg_best.fit(x_train_all, y_train_all)\n",
    "\n",
    "def lenet_model2():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(6696, input_shape=(300,), activation='relu'))\n",
    "    model.add(Dropout(0.5))    \n",
    "    model.add(Dense(3348, activation = 'linear'))\n",
    "#     opt = SGD(lr=0.01, momentum=0.9)\n",
    "    model.compile(Adam(lr = .01), loss = 'mse')\n",
    "    return model\n",
    "\n",
    "model = lenet_model2()\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "model.fit(x_train_all,y_train_all, epochs = 200, shuffle = 1, batch_size = 5000, verbose = 1)\n",
    "\n",
    "\n",
    "# y_test_pred = reg_best.predict(x_test)\n",
    "y_test_pred = model.predict(x_test)\n",
    "\n",
    "ranks = []\n",
    "for i in range(2000):\n",
    "    print(i)\n",
    "    tmp = np.zeros(2000)\n",
    "    for j in range(2000):\n",
    "        tmp[j] = get_dif(y_test_pred[i], y_test[j])\n",
    "        \n",
    "#     i_sorted = np.argsort(tmp)[::-1]\n",
    "    ranks.append(tmp)\n",
    "    \n",
    "# test_distances = dist_matrix(y_test_pred, y_test)\n",
    "test_distances = ranks\n",
    "pred_rows = []\n",
    "\n",
    "\n",
    "for i in range(num_test):\n",
    "    test_dist_idx = list(np.argsort(test_distances[i]))\n",
    "    top_20 = test_dist_idx[:20]\n",
    "    row = [\"%d.jpg\" % i for i in test_dist_idx[:20]]\n",
    "    pred_rows.append(\" \".join(row))\n",
    "\n",
    "with open(\"test_submission_NN_AllFeatures.csv\", \"w\") as f:\n",
    "    f.write(\"Descritpion_ID,Top_20_Image_IDs\\n\")\n",
    "    for i, row in enumerate(pred_rows):\n",
    "        f.write(\"%d.txt,%s\\n\" % (i, row))\n",
    "\n",
    "print(\"Output written!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
